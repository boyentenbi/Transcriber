{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "No module named pynlpl.lm.lm",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-54f483bd5daa>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mpynlpl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlm\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mlm\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mMultiTree\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mMT\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtime\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: No module named pynlpl.lm.lm"
     ]
    }
   ],
   "source": [
    "from copy import deepcopy\n",
    "import nltk\n",
    "import numpy as np\n",
    "import pynlpl.lm.lm as lm\n",
    "import MultiTree as MT\n",
    "from time import time\n",
    "from decoding import get_best_paths, power, times, divide"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%load_ext line_profiler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(40,)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "measures = np.load(\"phon_measures.npy\")\n",
    "measures.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = lm.ARPALanguageModel('lang_model/3-gram-3.ARPA', base_e = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-2.647338"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.scoreword(\"the\", history = (\"hello\",\"i\", \"am\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def pr_lang(x, y):\n",
    "    return 10.**(model.scoreword(x, history = tuple(y.word_hist)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "phon_probs_ = np.load(\"phoneme_probs.npy\")\n",
    "phon_probs_.shape\n",
    "\n",
    "# for i in range(phon_probs_.shape[0]):\n",
    "#     phon_probs_[i][39] /= 9.\n",
    "#     row_sum = np.sum(phon_probs_[i])\n",
    "#     phon_probs_[i] /= row_sum\n",
    "phon_probs = phon_probs_[30:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-W-AH-T-IH-Z-DHAH-S-TH-OW-D-L-AW-M-B-AY-\n"
     ]
    }
   ],
   "source": [
    "from process_data_temp import interpret_probs\n",
    "print \"\".join(interpret_probs(phon_probs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "phon_dict = {MT.phonemes[k] : k for k in range(39)}\n",
    "phon_dict[\"-\"] = 39"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def pr_ctc(phon, t):\n",
    "    phon_idx = phon_dict[phon]\n",
    "    return phon_probs[t-1][phon_idx] #/ measures[phon_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First 10 items in S are:\n",
      "[[u'the' u'7.615819']\n",
      " [u'to' u'7.263301']\n",
      " [u'of' u'7.242066']\n",
      " [u'a' u'7.222159']\n",
      " [u'and' u'7.210459']\n",
      " [u'in' u'7.173804']\n",
      " [u'that' u'6.843069']\n",
      " [u\"'s\" u'6.838454']\n",
      " [u'for' u'6.83241']\n",
      " [u'on' u'6.775705']]\n",
      "Last 10 items in S are:\n",
      "[[u'blindness' u'3.281488']\n",
      " [u'devout' u'3.281488']\n",
      " [u'feasibility' u'3.281488']\n",
      " [u'succumbed' u'3.281488']\n",
      " [u'fitzpatrick' u'3.281488']\n",
      " [u'kidman' u'3.281261']\n",
      " [u'lenses' u'3.281261']\n",
      " [u'extremes' u'3.281261']\n",
      " [u'portrays' u'3.281033']\n",
      " [u'parkway' u'3.281033']]\n",
      "Original dictionary has 123455 words\n",
      "Small dictionary has 16000 words\n",
      "R has 18433 keys\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#removes stresses from vowels\n",
    "\n",
    "phons = [u'AA',u'AE',u'AH',u'AO',u'AW',u'AY',u'B',u'CH',\n",
    "u'D',u'DH',u'EH',u'ER',u'EY',u'F',u'G',u'HH',u'IH',u'IY',u'JH',\n",
    "u'K',u'L',u'M',u'N',u'NG',u'OW',u'OY',u'P',u'R',u'S',u'SH',u'T',\n",
    "u'TH',u'UH', u'UW',u'V',u'W',u'Y',u'Z',u'ZH', u'-']\n",
    "\n",
    "phoneme_dict = dict(zip(phons, range(40)))\n",
    "\n",
    "MAX_WORDS = 16000\n",
    "S = np.load(\"unigram_scores.npz\")[\"arr_0\"]\n",
    "print \"First 10 items in S are:\"\n",
    "print S[:10]\n",
    "print \"Last 10 items in S are:\"\n",
    "print S[MAX_WORDS-10:MAX_WORDS]\n",
    "D = nltk.corpus.cmudict.dict()\n",
    "print \"Original dictionary has {} words\".format(len(D))\n",
    "D_small = {k: D[k] for k in S[:MAX_WORDS,0]}\n",
    "print \"Small dictionary has {} words\".format(len(D_small))\n",
    "\n",
    "\n",
    "def strip_stresses(phoneme):\n",
    "    main_phon = phoneme[:-1]\n",
    "    last_letter = phoneme[-1]\n",
    "    if last_letter.isnumeric():\n",
    "        return main_phon\n",
    "    else:\n",
    "        return phoneme\n",
    "    \n",
    "def obtain_phonemes(sentence, dictionary):\n",
    "    sentence_phonemes = []\n",
    "    for word in sentence:\n",
    "        word = word.lower()\n",
    "        if word in dictionary:\n",
    "            #nltk.corpus.cmudict.dict() gives a list of lists, giving both\n",
    "            #the american and english pronounciations\n",
    "            phonemes = dictionary[word][0]\n",
    "            stripped_phonemes = map(strip_stresses, phonemes)\n",
    "            sentence_phonemes.extend(stripped_phonemes)\n",
    "        else:\n",
    "            #print \"{} is not in the dictionary, can't get phonemes!\".format(word)\n",
    "            return None\n",
    "    return sentence_phonemes\n",
    "\n",
    "R = {}\n",
    "# assert len(D.items()) ==len(set([\" \".join([strip_stresses(phon) for phon in h]) for homophones in D.values() for h in homophones]))\n",
    "for word, hs in D_small.items():\n",
    "    for h in hs:\n",
    "        stripped = [strip_stresses(phon) for phon in h]\n",
    "        phon_string = \" \".join(stripped)\n",
    "        if R.get(phon_string):\n",
    "            R[phon_string].append(word)\n",
    "        else:\n",
    "            R[phon_string] = [deepcopy(word)]\n",
    "# for a, b in R.items():\n",
    "#     if a not in [\" \".join([strip_stresses(p) for p in D[x][0]]) for x in [\"what\", \"is\", \"the\", \"phone\", \"number\"] ]:\n",
    "#         del R[a]\n",
    "\n",
    "print \"R has {} keys\".format(len(R.keys()))\n",
    "print \"\"\n",
    "Rkeys = R.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[u'T',\n",
       "  u'AH0',\n",
       "  u'B',\n",
       "  u'ER2',\n",
       "  u'K',\n",
       "  u'Y',\n",
       "  u'AH0',\n",
       "  u'L',\n",
       "  u'OW1',\n",
       "  u'S',\n",
       "  u'IH0',\n",
       "  u'S'],\n",
       " [u'T',\n",
       "  u'UW0',\n",
       "  u'B',\n",
       "  u'ER2',\n",
       "  u'K',\n",
       "  u'Y',\n",
       "  u'AH0',\n",
       "  u'L',\n",
       "  u'OW1',\n",
       "  u'S',\n",
       "  u'AH0',\n",
       "  u'S'],\n",
       " [u'T',\n",
       "  u'UW0',\n",
       "  u'B',\n",
       "  u'ER2',\n",
       "  u'K',\n",
       "  u'Y',\n",
       "  u'UW0',\n",
       "  u'L',\n",
       "  u'OW1',\n",
       "  u'S',\n",
       "  u'AH0',\n",
       "  u'S']]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "D[\"tuberculosis\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000\n",
      "2000\n",
      "3000\n",
      "4000\n",
      "5000\n",
      "6000\n",
      "7000\n",
      "8000\n",
      "9000\n",
      "10000\n",
      "11000\n",
      "12000\n",
      "13000\n",
      "14000\n",
      "15000\n",
      "16000\n",
      "17000\n",
      "18000\n"
     ]
    }
   ],
   "source": [
    "ext_store = {}\n",
    "n_done = 0\n",
    "for spaced_phons, homs in deepcopy(R.items()):\n",
    "    if homs == [\"T UW M ER Z\"]:\n",
    "        print spaced_phons\n",
    "    phon_list = spaced_phons.split(\" \")\n",
    "    for i in range(len(phon_list)):\n",
    "        sublist = phon_list[:i]\n",
    "        key = \" \".join(sublist)\n",
    "        if key in ext_store.keys():\n",
    "            for hom in homs:\n",
    "                if not hom in ext_store[key]:\n",
    "                    ext_store[key].append(hom)\n",
    "        else:\n",
    "            ext_store[key] = deepcopy(homs)\n",
    "    n_done +=1\n",
    "    if n_done % 1000 == 0:\n",
    "        print n_done\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'tomorrow',\n",
       " u'tumor',\n",
       " u'tuna',\n",
       " u'tumors',\n",
       " u'tucson',\n",
       " u'tuned',\n",
       " u'tunes',\n",
       " u'tuesday',\n",
       " u'tuberculosis',\n",
       " u'tubes',\n",
       " u'tumultuous',\n",
       " u'tooth',\n",
       " u'toulouse',\n",
       " u'tube',\n",
       " u'tune',\n",
       " u'tool',\n",
       " u'tomb',\n",
       " u'tools',\n",
       " u'tonight',\n",
       " u'today',\n",
       " u'tunisia']"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ext_store[\"T UW\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[u'T',\n",
       "  u'AH0',\n",
       "  u'B',\n",
       "  u'ER2',\n",
       "  u'K',\n",
       "  u'Y',\n",
       "  u'AH0',\n",
       "  u'L',\n",
       "  u'OW1',\n",
       "  u'S',\n",
       "  u'IH0',\n",
       "  u'S'],\n",
       " [u'T',\n",
       "  u'UW0',\n",
       "  u'B',\n",
       "  u'ER2',\n",
       "  u'K',\n",
       "  u'Y',\n",
       "  u'AH0',\n",
       "  u'L',\n",
       "  u'OW1',\n",
       "  u'S',\n",
       "  u'AH0',\n",
       "  u'S'],\n",
       " [u'T',\n",
       "  u'UW0',\n",
       "  u'B',\n",
       "  u'ER2',\n",
       "  u'K',\n",
       "  u'Y',\n",
       "  u'UW0',\n",
       "  u'L',\n",
       "  u'OW1',\n",
       "  u'S',\n",
       "  u'AH0',\n",
       "  u'S']]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "D_small[\"tuberculosis\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from decoding import DecodeInfo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "89"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "T = len(phon_probs)\n",
    "T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pr_nb_store = [{} for i in range(T)]\n",
    "pr_b_store = [{} for i in range(T)]\n",
    "\n",
    "p_ctc = pr_ctc\n",
    "p_lang = pr_lang\n",
    "def pr_nb(t, y):\n",
    "        if y in pr_nb_store[t]:\n",
    "            return pr_nb_store[t][y]\n",
    "        else:\n",
    "            return 0.\n",
    "\n",
    "def pr_b(t, y):\n",
    "    if y in pr_b_store[t]:\n",
    "        return pr_b_store[t][y]\n",
    "    else:\n",
    "        return 0.\n",
    "\n",
    "def pr_nb_add(t, y, val):\n",
    "    if y in pr_nb_store[t]:\n",
    "        pr_nb_store[t][y] += val\n",
    "    else:\n",
    "        pr_nb_store[t][y] = val\n",
    "\n",
    "def pr_b_add(t, y, val):\n",
    "    if y in pr_b_store[t]:\n",
    "        pr_b_store[t][y] += val\n",
    "    else:\n",
    "        pr_b_store[t][y] = val\n",
    "\n",
    "#extension probability of adding a phoneme k\n",
    "# CHANGED THIS\n",
    "def p_ext(k, y, t):\n",
    "    if y.last_phoneme() == k:\n",
    "        p_last = pr_b(t-1, y)\n",
    "    else:\n",
    "        p_last = pr_nb(t-1, y) + pr_b(t-1, y)            \n",
    "    \n",
    "    a = p_ctc(k,t)\n",
    "    b = p_trans(k, y)\n",
    "    bp = power(b,beta)\n",
    "    \n",
    "    return times([a, bp, p_last])\n",
    "\n",
    "#derived phoneme transition prob from language model\n",
    "def p_trans(k, y):\n",
    "    if y.tree.lookup([k]) == None:\n",
    "        return 0.\n",
    "    else:\n",
    "        denom_leaves = ext_store.get(\" \".join(y.partial_word), [])\n",
    "        numer_leaves = ext_store.get(\" \".join(y.partial_word) + \" \" + k, [])\n",
    "        denom_ps = [p_lang(x, y) for x in denom_leaves]\n",
    "        numer_ps = [p_lang(x, y) for x in numer_leaves]\n",
    "        denom = np.sum(np.asarray(denom_ps))\n",
    "        numer = np.sum(np.asarray(numer_ps))\n",
    "        return divide(numer, denom)\n",
    "\n",
    "#penalty probability for converting to a homophone\n",
    "def p_homophone(word, y):\n",
    "    denom = np.sum(np.asarray([p_lang(x, y) for x in ext_store.get(\" \".join(y.partial_word), [])]))\n",
    "    numer = p_lang(word, y)\n",
    "    return divide(numer, denom)\n",
    "    \n",
    "\n",
    "def test_beam_search(T, p_ctc, p_lang, width, beta):\n",
    "    \n",
    "    #initialising non-blank prob\n",
    "    empty_str = DecodeInfo()\n",
    "    pr_nb_store[0][empty_str] = 1.\n",
    "    pr_b_store[0][empty_str] = 0.\n",
    "    \n",
    "    #most probable sequences\n",
    "    B = [empty_str]\n",
    "    \n",
    "    for t in range(1, T+1):\n",
    "            B_hat = get_best_paths(B, pr_b, pr_nb, width, t-1)\n",
    "            print \"\\n\".join(str(elem) + \" : {}\".format(pr_b(t-1, elem) + pr_nb(t-1, elem)) for elem in B_hat) + \"\\n\\n\"\n",
    "            B = []\n",
    "            for y in B_hat:\n",
    "                if not y.empty():\n",
    "                    pr_nb_store[t][y]= times([pr_nb(t-1, y), p_ctc(y.last_phoneme(), t)])\n",
    "                    \n",
    "                    #dont think we should have this - paths which decode to the same thing are accounted for in the\n",
    "                    #loop below\n",
    "#                     if y.one_fewer() in B_hat:\n",
    "#                         pr_nb_store[t][y] += p_ext(y.last_phoneme(), y.one_fewer(), t)\n",
    "\n",
    "                pr_b_store[t][y] = times(pr_b(t-1,y) + pr_nb(t-1, y), p_ctc(\"-\", t))\n",
    "                \n",
    "                B.append(y)\n",
    "                \n",
    "                for k in MT.phonemes:\n",
    "\n",
    "                    #if adding a phoneme k is valid\n",
    "                    if k in y.tree.branches:\n",
    "\n",
    "                        pr_plus = p_ext(k, y, t)\n",
    "\n",
    "                        #initialising y_new = y+k\n",
    "                        y_new = DecodeInfo(orig = y)\n",
    "                        y_new.add_phon(k)\n",
    "\n",
    "                        conversion_penalty_prob = 0.\n",
    "                        #handles conversion from phoneme string to word\n",
    "                        for word in y_new.tree.leaf:\n",
    "\n",
    "                            #converted_y converts phoneme part of y_new into a new word\n",
    "                            converted_y = DecodeInfo()\n",
    "                            converted_y.word_hist = deepcopy(y_new.word_hist)\n",
    "                            converted_y.word_hist.append(word)\n",
    "                            pr_b_add(t, converted_y, 0.)\n",
    "                            pr_nb_add(t, converted_y, times(pr_plus, power(p_homophone(word, y_new), beta)))\n",
    "                            B.append(converted_y)\n",
    "                            conversion_penalty_prob += p_homophone(word, y_new)\n",
    "\n",
    "                        #penalises non-converted string\n",
    "                        pr_nb_add(t, y_new, times(pr_plus, power(1.0 - conversion_penalty_prob, beta)))\n",
    "                        B.append(y_new)\n",
    "\n",
    "    return (B, pr_b_store, pr_nb_store)#get_best_paths(B, pr_b_store, pr_nb_store, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "beta = 0.05"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| : 1.0\n",
      "\n",
      "\n",
      "| : 0.995358049646\n",
      "o'| : 0.0\n",
      "|ER : 0.0\n",
      "|TH : 0.0\n",
      "ou| : 0.0\n",
      "i| : 0.0\n",
      "|EY : 0.0\n",
      "|AW : 0.0\n",
      "eaux| : 0.0\n",
      "|S : 0.0\n",
      "\n",
      "\n",
      "| : 0.991759808681\n",
      "i|V : 0.0\n",
      "i|W : 0.0\n",
      "i|T : 0.0\n",
      "i|R : 0.0\n",
      "i|S : 0.0\n",
      "i|P : 0.0\n",
      "i|Z : 0.0\n",
      "i|Y : 0.0\n",
      "i|F : 0.0\n",
      "\n",
      "\n",
      "| : 0.808761114311\n",
      "i|V : 0\n",
      "i|W : 0\n",
      "i|T : 0\n",
      "i xie| : 0.0\n",
      "i|R : 0\n",
      "i|S : 0\n",
      "i|P : 0\n",
      "i suh| : 0.0\n",
      "i roi| : 0.0\n",
      "\n",
      "\n",
      "| : 0.0558780578637\n",
      "i|V : 0\n",
      "i|W : 0\n",
      "i|T : 0\n",
      "i xie| : 0\n",
      "i|R : 0\n",
      "i|S : 0\n",
      "i|P : 0\n",
      "i suh| : 0\n",
      "i roi| : 0\n",
      "\n",
      "\n",
      "| : 0.00211543263787\n",
      "i|V : 0\n",
      "i|W : 0\n",
      "i|T : 0\n",
      "i xie| : 0\n",
      "i|R : 0\n",
      "i|S : 0\n",
      "i|P : 0\n",
      "i suh| : 0\n",
      "i roi| : 0\n",
      "\n",
      "\n",
      "| : 0.00206998779851\n",
      "i|V : 0\n",
      "i|W : 0\n",
      "i|T : 0\n",
      "i xie| : 0\n",
      "i|R : 0\n",
      "i|S : 0\n",
      "i|P : 0\n",
      "i suh| : 0\n",
      "i roi| : 0\n",
      "\n",
      "\n",
      "*** KeyboardInterrupt exception caught in code being profiled."
     ]
    }
   ],
   "source": [
    "%lprun -f p_lang test_beam_search(T, pr_ctc, pr_lang, 10, beta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.8625862530573387"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pr_b_store[2][DecodeInfo()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pr_nb_store = [{}]*(T+1)\n",
    "pr_b_store = [{}]*(T+1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DecodeInfo().empty()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "nan"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "power(-1,0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
