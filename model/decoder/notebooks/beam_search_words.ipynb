{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import nltk\n",
    "\n",
    "n_paths = 5\n",
    "alpha = 1\n",
    "beta = 0.1\n",
    "\n",
    "#phons = \"abcdefghijklmnopqrstuvwxyz -\"\n",
    "\n",
    "\n",
    "\n",
    "phoneme_dict =   {#' ': 0,\n",
    "                     u'AA': 0,\n",
    "                     u'AE': 1,\n",
    "                     u'AH': 2,\n",
    "                     u'AO': 3,\n",
    "                     u'AW': 4,\n",
    "                     u'AY': 5,\n",
    "                     u'B': 6,\n",
    "                     u'CH': 7,\n",
    "                     u'D': 8,\n",
    "                     u'DH': 9,\n",
    "                     u'EH': 10,\n",
    "                     u'ER': 11,\n",
    "                     u'EY': 12,\n",
    "                     u'F': 13,\n",
    "                     u'G': 14,\n",
    "                     u'HH': 15,\n",
    "                     u'IH': 16,\n",
    "                     u'IY': 17,\n",
    "                     u'JH': 18,\n",
    "                     u'K': 19,\n",
    "                     u'L': 20,\n",
    "                     u'M': 21,\n",
    "                     u'N': 22,\n",
    "                     u'NG': 23,\n",
    "                     u'OW': 24,\n",
    "                     u'OY': 25,\n",
    "                     u'P': 26,\n",
    "                     u'R': 27,\n",
    "                     u'S': 28,\n",
    "                     u'SH': 29,\n",
    "                     u'T': 30,\n",
    "                     u'TH': 31,\n",
    "                     u'UH': 32,\n",
    "                     u'UW': 33,\n",
    "                     u'V': 34,\n",
    "                     u'W': 35,\n",
    "                     u'Y': 36,\n",
    "                     u'Z': 37,\n",
    "                     u'ZH': 38,\n",
    "                        \"-\": 39}\n",
    "#removes stresses from vowels\n",
    "def strip_stresses(phoneme):\n",
    "    main_phon = phoneme[:-1]\n",
    "    last_letter = phoneme[-1]\n",
    "    if last_letter.isnumeric():\n",
    "        return main_phon\n",
    "    else:\n",
    "        return phoneme\n",
    "def obtain_phonemes(sentence, dictionary):\n",
    "    sentence_phonemes = []\n",
    "    for word in sentence:\n",
    "        word = word.lower()\n",
    "        if word in dictionary:\n",
    "            #nltk.corpus.cmudict.dict() gives a list of lists, giving both\n",
    "            #the american and english pronounciations\n",
    "            phonemes = dictionary[word][0]\n",
    "            stripped_phonemes = map(strip_stresses, phonemes)\n",
    "            sentence_phonemes.extend(stripped_phonemes)\n",
    "        else:\n",
    "            #print \"{} is not in the dictionary, can't get phonemes!\".format(word)\n",
    "            return None\n",
    "    return sentence_phonemes\n",
    "phons = phoneme_dict.keys()\n",
    "D = nltk.corpus.cmudict.dict()\n",
    "R = {k: \" \".join(obtain_phonemes([k], D)) for k in D.keys()}\n",
    "\n",
    "def stimes(factors):\n",
    "    return np.exp(np.sum(np.log(factors)))\n",
    "   \n",
    "def sexp(base, exp):\n",
    "    return np.exp(np.log(base) * exp)\n",
    "\n",
    "\n",
    "def sdiv(a,b):\n",
    "    return np.exp(np.log(a)-np.log(b))\n",
    "\n",
    "def phonemes_to_homophones(p):\n",
    "    return R.get(p)\n",
    "\n",
    "# Compute the transition probability\n",
    "def get_p_w_p_star(w, p):\n",
    "    # It is critical that the regex has \\s at the end.\n",
    "    # This prevents p from matching with words for which it is a non-strict prefix\n",
    "    wprimes = [word for word in words if re.match(r\"^\"+p+\"\\s\", \" \".join(obtain_phonemes(word)))]\n",
    "    p_w_wprimes = [lm(wprime, w) for wprime in wprimes]\n",
    "    return np.sum(p_w_wprimes)\n",
    "\n",
    "def last(w,p):\n",
    "    list_w = w.split()\n",
    "    list_p = p.split()\n",
    "    if list_p:\n",
    "        return list_p[-1]\n",
    "    else:\n",
    "        return list_w[-1]\n",
    "def hat(w,p):\n",
    "    if p:\n",
    "        return (w, \" \".join(p.split(\" \")[:-1]))\n",
    "    elif w:\n",
    "        return (\" \".join(obtain_phonemes(w.split(\" \"))[:-1]),p)\n",
    "    else:\n",
    "        return w,p\n",
    "    \n",
    "def top_keys(d, k, beta):\n",
    "    sentences = [a[0] for a in d.keys()]\n",
    "    logprobs = np.asarray(d.values())\n",
    "    string_lens = np.asarray([len(sentence)+1 for sentence in sentences], dtype=float)\n",
    "    \n",
    "    top_k_idxs = argsort(logprobs / string_len)[-k:] # dividing makes them greater because they're negative!\n",
    "    top_k_keys = [d.keys()[i] for i in top_k_idxs]\n",
    "    return top_k_keys\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def beam_search(phon_probs):\n",
    "    Z = [[] for t in range(T)]\n",
    "    p_b = [{} for t in range(T)]\n",
    "    p_nb = [{} for t in range(T)]\n",
    "    p = [{} for t in range(T)]\n",
    "    p_w_p = [{} for t in range(T)]\n",
    "    # Just an initialization that works...\n",
    "    p_nb[0] = {(\"\",\"\"):1.}\n",
    "    p_b[0] = {(\"\",\"\"): 0.}\n",
    "    p[0] = {(\"\",\"\"):1.}\n",
    "\n",
    "    Z[0] = [(\"\", \"\")]\n",
    "    T = phon_probs.shape[0]\n",
    "\n",
    "    # Iterate over input timesteps\n",
    "    for t in range(1, T+1):\n",
    "        print \"t={}\".format(t)\n",
    "        # Consider all of the (w,p) pairs which were candidates given data up to time t-1\n",
    "        # and compute the probabilities of 'some' new strings given data up to time t\n",
    "        # 'some' = the current strings and 1-phoneme extensions of them\n",
    "        for (w,p) in Z[t-1]:\n",
    "            print \"considering ({},{})\"\n",
    "            # We need to compute:\n",
    "            \n",
    "            # 1. p_nb[t][(w,p)]\n",
    "            # 2. p_b[t][(w,p)]\n",
    "            # 3. p_nb[t][(w,p + k)] for all k not blank. Remembering that this may lead to a new word.\n",
    "           \n",
    "            # Consider the extension by a blank phoneme\n",
    "            # This always leaves (w,p) as it was\n",
    "            # So we are assigning p_nb[t][(w,p)] and p_b[t][(w,p)]\n",
    "            p_nb_t_add = \n",
    "            if p_nb[t].get((w,p)):\n",
    "                \n",
    "            p_nb[t][(w,p)] = stimes([p_nb[t-1][(w,p)], phon_probs[t, phoneme_dict[last(w,p)]]]) if (w,p)!=(\"\",\"\") else 0. \n",
    "            if hat(w,p) in Z[t-1]:\n",
    "                \n",
    "            print \"p_b[{}] =\".format(t), p_b[t]\n",
    "            print \"p[{}] =\".format(t-1), p[t-1]\n",
    "            p_b[t][(w,p)] =  stimes([p[t-1][(w,p)], phon_probs[t, phoneme_dict[\"-\"]]])\n",
    "            \n",
    "                \n",
    "            # Consider extensions of the uncontracted thing by one phoneme\n",
    "            for k in phons[:-1]:\n",
    "                \n",
    "                # When the extension definitely adds a phoneme to the contraction\n",
    "                # all the probs are nb because we aren't adding blank!\n",
    "                if k!=\"-\" and k != last(w,p):\n",
    "\n",
    "                    # if p+k can be read as a word v, compute the probability of (wv,0) using the lm\n",
    "                    # for every such v\n",
    "                    # Note that (-p_w_p_star[(w,p)])*alpha exactly cancels the same term in a prev computation of p[t-1][(w,p)]\n",
    "                    # This is because (unlogarithmed) p[t-1] can be factored into acoustic and language factors\n",
    "                    # where exp(p_w_p_star[(w,p)]*alpha) is the language factor\n",
    "                    # The new language model factor is just the lm evaluated on v and the sentence\n",
    "                    for h in R.get(p+\" \"+k):\n",
    "                        p_nb[t][(w+\" \"+h, \"\")] = stimes([phon_probs[t, phoneme_dict[k]],\n",
    "                                                  p[t-1][(w,p)],\n",
    "                                                  sexp(sdiv(lm(h, w),p_w_p_star[(w,p)]), alpha)])\n",
    "                        \n",
    "                        p_b[t][(w+\" \"+h, \"\")] =  0.\n",
    "                              \n",
    "                    # Compute the probability of (w,p+k) under the language model, NOT CONSIDERING THE ACOUSTIC MODEL\n",
    "                    # This is the sum of probabilities of wx where p is a strict prefix of every x\n",
    "                    p_w_p_star[(w, p+k)] = get_p_w_p_star(w, p+k)\n",
    "                            \n",
    "                    p_nb[t][(w,p+k)] = stimes([phon_probs[t, phoneme_dict[k]],\n",
    "                                        p[t-1][(w,p)],\n",
    "                                        sexp(sdiv(p_w_p_star[(w,p+k)], p_w_p_star[(w,p)]), alpha)])\n",
    "                            \n",
    "                # When extension is the same as the final phoneme\n",
    "                # the extension only extends y if the final phoneme in the uncontracted thing is blank\n",
    "                elif k!=\"-\" and k==last(w,p):\n",
    "                    p_nb[t][(w,p+k)] = stimes([phon_probs[t, phoneme_dict[k]],\n",
    "                                       p_b[t-1][(w,p)],\n",
    "                                        sexp(sdiv(p_w_p_star[(w,p+k)], p_w_p_star[(w,p)]), alpha)])\n",
    "                else: # k == \"-\"\n",
    "                    continue\n",
    "                \n",
    "            # Set the probability of a string given input data up to time t as the sum of the marginals over t'th model out\n",
    "            p[t] = {y: p_b[t][y] + p_nb[t][y] for y in p_b[t].keys()}\n",
    "            # Take the top k strings \n",
    "            Z[t] = top_keys(p[t], n_paths, beta)\n",
    "    return Z[T]\n",
    "    # Don't forget to throw away the p's when you're done!\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t=1\n",
      "p_b[1] = {}\n",
      "p[0] ="
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "string index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-36-337410c12ac7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# phon_probs =\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mbeam_search\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"phoneme_probs.npy\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-35-f78a8728e8cf>\u001b[0m in \u001b[0;36mbeam_search\u001b[0;34m(phon_probs)\u001b[0m\n\u001b[1;32m     22\u001b[0m             \u001b[0mp_nb\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mp_nb\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mphon_probs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mphoneme_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m!=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;31m# plus some other stuff???\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m             \u001b[0;32mprint\u001b[0m \u001b[0;34m\"p_b[{}] =\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp_b\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m             \u001b[0;32mprint\u001b[0m \u001b[0;34m\"p[{}] =\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m             \u001b[0mp_b\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m  \u001b[0mp\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mphon_probs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mphoneme_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"-\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: string index out of range"
     ]
    }
   ],
   "source": [
    "# phon_probs = \n",
    "beam_search(np.load(\"phoneme_probs.npy\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
