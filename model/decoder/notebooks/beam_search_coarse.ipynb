{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import nltk\n",
    "import re\n",
    "\n",
    "\n",
    "phons = [u'AA',u'AE',u'AH',u'AO',u'AW',u'AY',u'B',u'CH',\n",
    "u'D',u'DH',u'EH',u'ER',u'EY',u'F',u'G',u'HH',u'IH',u'IY',u'JH',\n",
    "u'K',u'L',u'M',u'N',u'NG',u'OW',u'OY',u'P',u'R',u'S',u'SH',u'T',\n",
    "u'TH',u'UH', u'UW',u'V',u'W',u'Y',u'Z',u'ZH', u'-']\n",
    "\n",
    "phoneme_dict =   dict(zip(phons, range(40)))\n",
    "\n",
    "\n",
    "#removes stresses from vowels\n",
    "def strip_stresses(phoneme):\n",
    "    main_phon = phoneme[:-1]\n",
    "    last_letter = phoneme[-1]\n",
    "    if last_letter.isnumeric():\n",
    "        return main_phon\n",
    "    else:\n",
    "        return phoneme\n",
    "def obtain_phonemes(sentence, dictionary):\n",
    "    sentence_phonemes = []\n",
    "    for word in sentence:\n",
    "        word = word.lower()\n",
    "        if word in dictionary:\n",
    "            #nltk.corpus.cmudict.dict() gives a list of lists, giving both\n",
    "            #the american and english pronounciations\n",
    "            phonemes = dictionary[word][0]\n",
    "            stripped_phonemes = map(strip_stresses, phonemes)\n",
    "            sentence_phonemes.extend(stripped_phonemes)\n",
    "        else:\n",
    "            #print \"{} is not in the dictionary, can't get phonemes!\".format(word)\n",
    "            return None\n",
    "    return sentence_phonemes\n",
    "\n",
    "D = nltk.corpus.cmudict.dict()\n",
    "D[\"what\"]\n",
    "R = {}\n",
    "\n",
    "# assert len(D.items()) ==len(set([\" \".join([strip_stresses(phon) for phon in h]) for homophones in D.values() for h in homophones]))\n",
    "for word, hs in D.items():\n",
    "    for h in hs:\n",
    "        stripped = [strip_stresses(phon) for phon in h]\n",
    "        phon_string = \" \".join(stripped)\n",
    "        if R.get(phon_string):\n",
    "            R[phon_string].append(word)\n",
    "        else:\n",
    "            R[phon_string] = [word]\n",
    "for a, b in R.items():\n",
    "    if a not in [\" \".join([strip_stresses(p) for p in D[x][0]]) for x in [\"what\", \"is\", \"the\", \"phone\", \"number\"] ]:\n",
    "        del R[a]\n",
    "print \"R has {} keys\".format(len(R.keys()))\n",
    "\n",
    "\n",
    "def stimes(*factors):\n",
    "    if 0. in factors:\n",
    "        return 0.\n",
    "    else: \n",
    "        return np.exp(np.sum(np.log(factors)))\n",
    "def sexp(base, exp):\n",
    "    if base ==0.:\n",
    "        return 0.\n",
    "    else:\n",
    "        return np.exp(exp * np.log(base))\n",
    "def sexp_arr(base, exps):\n",
    "    \n",
    "    return [0. if b==0. else np.exp(np.log(b) * e) for b,e in zip(base,exps) ]\n",
    "def sdiv(a,b):\n",
    "    if a == 0.:\n",
    "        return 0.\n",
    "    return np.exp(np.log(a)-np.log(b))\n",
    "\n",
    "def lm_raw(w):\n",
    "    \n",
    "    return np.exp(0.)\n",
    "# Compute the transition probability\n",
    "\n",
    "\n",
    "def get_pr_lang_ph(y):\n",
    "    return 1.\n",
    "\n",
    "    \n",
    "def top_SPs(prs, B, k, beta):\n",
    "    sentences = [y.w for y in B]\n",
    "    probs = np.asarray([prs[y.sh] for y in B])\n",
    "    string_lens = np.asarray([len(sentence)+1 for sentence in sentences], dtype=float)\n",
    "    scores = sexp_arr(probs,1./string_lens)\n",
    "    #print scores\n",
    "    top_k_idxs = np.argsort(scores)[-k:] # dividing makes them greater because they're negative!\n",
    "    return [B[i] for i in top_k_idxs]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def beam_search_coarse(acoustic_score, language_score, alpha, beta, frame_level_probs, n_paths):\n",
    "    # Initialize the candidate-score dict with the empty string\n",
    "    candidates = {\"\":0.}\n",
    "    # Iterate over input timesteps and possibly grow the tree at each\n",
    "    for t in range(T):\n",
    "        \n",
    "        # Retrieve the best paths so far\n",
    "        top_candidates = dict(sorted(candidates.items(), key= lambda x: x[1])[:n_paths])\n",
    "        \n",
    "        # Check each path for whether growing it will improve its score\n",
    "        for s,score in top_candidates.items():\n",
    "            \n",
    "            # We don't want to iterate over every word because there are a ton of them. \n",
    "            # It's probably better to let the phones dictate which ones we search\n",
    "            # So we iterate through phoneme extensions and score them \n",
    "            # Check every extension allowed by the lm\n",
    "            for word in language:\n",
    "                s_new = s+\" \"+word\n",
    "                if s_new in candidates.keys() break\n",
    "                    \n",
    "                new_phonemes = code_phonemes(s_new)\n",
    "                new_score = acoustic_score(frame_level_probs, new_phonemes) \\\n",
    "                            + alpha*language_score(s_new) \\\n",
    "                            + beta*len(s_new.split(\" \"))\n",
    "                if new_score >= score:\n",
    "                    candidates[s_new] = new_score"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
